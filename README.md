# ğŸ¶ğŸ± Proyecto Final CS2013: DogCatMLP
## Clasificador de Perros y Gatos â€” MLP desde C++
**Curso:** CS2013 ProgramaciÃ³n III â€” Universidad UTEC â€” 2025-1  
**Tema:** Red Neuronal Multicapa (MLP) Â· ClasificaciÃ³n Binaria Â· ImplementaciÃ³n desde cero

---

## ğŸ“ DescripciÃ³n del Proyecto

DogCatMLP es un clasificador binario de imÃ¡genes (perros vs. gatos) implementado desde cero en **C++**, usando una **Red Neuronal MultiCapa (MLP)** y entrenada mediante **Backpropagation** con **SGD**.

El sistema utiliza imÃ¡genes RAW de **64Ã—64 pÃ­xeles**, una arquitectura simple pero funcional, y una librerÃ­a interna (`utec/nn`) desarrollada como parte del curso.

---

## ğŸ“š Contenidos

- Datos generales
- Requisitos e InstalaciÃ³n
- 1. InvestigaciÃ³n TeÃ³rica
- 2. DiseÃ±o e ImplementaciÃ³n
- 3. EjecuciÃ³n y Pruebas
- 4. AnÃ¡lisis del Rendimiento
- 5. Trabajo en Equipo
- 6. Conclusiones
- 7. BibliografÃ­a
- Licencia

---

## ğŸ“Œ Datos generales

| Detalle | Valor                                                           |
|--------|-----------------------------------------------------------------|
| Tema | ClasificaciÃ³n Binaria de ImÃ¡genes usando Redes Neuronales (MLP) |
| Proyecto | DogCatMLP                                                       |
| Grupo | Grupo_ASWD                                                      |

### ğŸ‘¥ Integrantes

| Nombre del Alumno | CÃ³digo    | Rol              |
|-------------------|-----------|------------------|
| Fernando Espinoza | 202420465 | Unico Integrante |

---

## ğŸ“¦ Requisitos e InstalaciÃ³n

### Requisitos de Software

- **Compilador:** GCC 11+ (compatible con C++17)
- **Build System:** CMake 3.18+
- **Dependencias:** LibrerÃ­a interna `utec/nn`
- **Dataset:** Carpeta `../dataset/train/{dog,cat}` con imÃ¡genes RAW 64Ã—64

---

### âš™ï¸ Instrucciones de InstalaciÃ³n

```bash
# 1. Clonar el repositorio
git clone https://github.com/CS1103/proyecto-final-2025-2-asdw.git
cd EPIC3-ASWD

# 2. Configurar y compilar
mkdir build && cd build
cmake ..
make


---

# ğŸ“˜ 1. InvestigaciÃ³n TeÃ³rica

Esta etapa estableciÃ³ los fundamentos necesarios para la implementaciÃ³n de una red neuronal desde cero.

## Contenido Central

### ğŸ§  Historia y EvoluciÃ³n de las Redes Neuronales
Desde el PerceptrÃ³n y Adaline hasta las arquitecturas modernas.

### ğŸ—ï¸ Arquitectura MLP
- Capacidad de aproximaciÃ³n universal  
- Adecuada para datos aplanados (imÃ¡genes 64Ã—64 = 4096 pixeles)

### ğŸ”§ Algoritmos de Entrenamiento
- **Backpropagation**: cÃ¡lculo de gradientes  
- **SGD**: optimizaciÃ³n estocÃ¡stica  
- **Binary Cross Entropy (BCE)**: funciÃ³n de pÃ©rdida  

---

# ğŸ—ï¸ 2. DiseÃ±o e ImplementaciÃ³n

## 2.1 Arquitectura de la SoluciÃ³n

El modelo principal se encapsula en la clase **`DogCatClassifier`**, construida sobre la infraestructura de `utec/nn`.

### ğŸ§© Arquitectura de la Red MLP

| Capa | ConfiguraciÃ³n |
|------|---------------|
| Capa Densa (Input) | 4096 â†’ 64 |
| ActivaciÃ³n | ReLU |
| Capa Densa (Output) | 64 â†’ 1 |
| ActivaciÃ³n Final | Sigmoid |

### ğŸ› ï¸ Patrones de DiseÃ±o

- **Strategy:** optimizador SGD  
- **Factory:** creaciÃ³n de capas (Dense, ReLU, Sigmoid)

### ğŸ“ Estructura de Carpetas

proyecto-final-2025-2-asdw/  
â”œâ”€â”€ src/  
â”‚   â”œâ”€â”€ main.cpp  
â”œâ”€â”€ include/  
â”‚   â”œâ”€â”€ utec/
â”‚   â”‚   â”œâ”€â”€ nn/
â”‚   â”‚   â”‚   â”œâ”€â”€ neural_network.h
â”‚   â”‚   â”‚   â”œâ”€â”€ nn_activation.h
â”‚   â”‚   â”‚   â”œâ”€â”€ nn_dense.h
â”‚   â”‚   â”‚   â”œâ”€â”€ nn_interfaces.h
â”‚   â”‚   â”‚   â”œâ”€â”€ nn_loss.h
â”‚   â”‚   â”‚   â”œâ”€â”€ nn_optimizer.h
â”‚   â”‚   â”œâ”€â”€ algebra/
â”‚   â”‚   â”‚   â”œâ”€â”€ Tensor.h
â”œâ”€â”€ external/
â”‚   â”œâ”€â”€ image_loader.h
â”œâ”€â”€ dataset/  
â”‚   â”œâ”€â”€ train/  
â”‚   â”‚   â”œâ”€â”€ dog/  
â”‚   â”‚   â”œâ”€â”€ cat/  
â”‚   â”œâ”€â”€ test/  
â”‚   â”‚   â”œâ”€â”€ dog/  
â”‚   â”‚   â”œâ”€â”€ cat/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ DogCatClassifier.cpp
|   â”œâ”€â”€ DogCatClassifier.h
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ README.md

---

# ğŸ§ª 3. EjecuciÃ³n y Pruebas

### ğŸ¥ Demo
Video disponible en: `docs/demo.mp4`

### âš™ï¸ ParÃ¡metros de Entrenamiento

| ParÃ¡metro | Valor |
|-----------|--------|
| Ã‰pocas | 75 |
| Learning Rate | 0.005 |
| Train/Validation | 90% / 10% |
| DimensiÃ³n de Entrada | 4096 |
| Neuronas Ocultas | 64 |

---

# ğŸ“Š 4. AnÃ¡lisis del Rendimiento

### ğŸ“ˆ MÃ©tricas

| MÃ©trica | Valor |
|---------|--------|
| Iteraciones | 75 Ã©pocas |
| Tiempo total | *[Completar]* |
| PrecisiÃ³n de ValidaciÃ³n | *[Completar, ej: 72.7%]* |
| PÃ©rdida Final | *[Completar]* |

---

### âš–ï¸ Ventajas y Desventajas

| Aspecto | Ventaja | Desventaja |
|---------|----------|-------------|
| CÃ³digo | Ligero, dependencias mÃ­nimas | No usa BLAS o Eigen |
| Rendimiento | Inferencia rÃ¡pida | Entrenamiento sin paralelizaciÃ³n |


---

# ğŸ§  5. Conclusiones

### ğŸ† Logros
Se implementÃ³ un clasificador funcional basado en una **MLP desde cero en C++**, logrando una precisiÃ³n aproximada de **[ej: 72.7%]** en un dataset real.

### ğŸ“˜ Aprendizajes
- Entendimiento profundo del **Backpropagation**  
- Importancia de **normalizaciÃ³n** e **inicializaciÃ³n de pesos**

### ğŸ› ï¸ Recomendaciones
Para escalar el proyecto se sugiere optimizar memoria y cÃ³mputo usando:
- **BLAS**  
- **Eigen**  
- ParalelizaciÃ³n mediante **mini-batches**  

---

# ğŸ“š 6. BibliografÃ­a (Formato IEEE)

- Ringa Tech. (2021, 30 de noviembre). Â¿Pocos datos de entrenamiento? Prueba esta tÃ©cnica [Video]. YouTube. https://www.youtube.com/watch?v=9Dur_oUMGG8

---

# ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia **MIT**.  
Consulte el archivo `LICENSE` para mÃ¡s detalles.2